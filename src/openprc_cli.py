import argparse
import sys
from pathlib import Path

TEMPLATE_CONTENT = """
# Your new benchmark file!
# This was generated by `openprc create-benchmark`.
# Follow the instructions in the comments to create your custom analysis.

from pathlib import Path
import numpy as np
import h5py

# [Instruction] Step 1: Import the base classes
# Every benchmark requires these two components from the OpenPRC library.
# - BaseBenchmark: The interface that the pipeline uses to run your analysis.
# - BaseBenchmarkResult: The object that holds and saves your results.
#
# To make this import work during local development, you must add the 'src'
# directory of the OpenPRC project to your Python path in your main script:
#
# import sys
# from pathlib import Path
# # Assumes your script is in a subdirectory of the project root
# sys.path.append(str(Path(__file__).resolve().parent.parent / 'src'))
#
from analysis.benchmarks import BaseBenchmark, BaseBenchmarkResult


# [Instruction] Step 2: Create a result class for your benchmark
# This class defines what data your benchmark produces and how it gets saved.
class MyBenchmarkResult(BaseBenchmarkResult):
    # [Instruction] 2a: Update the constructor
    # Add arguments to __init__ to store the values your benchmark calculates.
    def __init__(self, experiment_dir: Path, my_metric_value: float):
        super().__init__(experiment_dir)
        self.my_metric_value = my_metric_value

    # [Instruction] 2b: Implement the save method
    # This method writes your results into a standard `metrics.h5` file.
    def save(self) -> Path:
        '''Saves the custom metrics to the project's metrics.h5 file.'''
        # The `get_metrics_path` helper from the base class creates the
        # `metrics` directory and returns the full path for you.
        metrics_path = self.get_metrics_path(filename="my_benchmark_metrics.h5")
        print(f"Updating {metrics_path} with results from {self.__class__.__name__}")

        with h5py.File(metrics_path, 'a') as f:
            my_grp = f.require_group('my_benchmark_results')
            my_grp['my_metric'] = self.my_metric_value
            # The `readout_path` and `simulation_path` properties are available
            # from the base class for context.
            my_grp.attrs['source_readout'] = str(self.readout_path)
            my_grp.attrs['source_simulation'] = str(self.simulation_path)

        return metrics_path


# [Instruction] Step 3: Create your custom benchmark class
# This class contains the main logic for your analysis.
class MyBenchmark(BaseBenchmark):
    '''Holds the result(s) from MyBenchmark and saves them to metrics.h5.'''
    # [Instruction] 3a: Implement the run method
    # The pipeline will call this method, providing the experiment directory.
    def run(self, experiment_dir: Path, **kwargs) -> MyBenchmarkResult:
        '''
        Loads data from the experiment, computes a custom metric,
        and returns the result object.
        '''
        print(f"Running {self.__class__.__name__} on {experiment_dir.name}")
        
        # [Instruction] 3b: Load the necessary data
        # You can access any file in the experiment directory.
        # The base result class provides helpers for standard files.
        temp_result = MyBenchmarkResult(experiment_dir, 0.0)
        
        # Example 1: Load from readout.h5
        print(f"Loading predictions from: {temp_result.readout_path}")
        with h5py.File(temp_result.readout_path, 'r') as f:
            group = 'validation' if 'validation' in f else 'training'
            target = f[f'{group}/target'][:] 
            prediction = f[f'{group}/prediction'][:]
            
        # Example 2: Load from simulation.h5
        print(f"Loading states from: {temp_result.simulation_path}")
        with h5py.File(temp_result.simulation_path, 'r') as f:
            # E.g., load the node positions over time
            node_positions = f['time_series/nodes/positions'][:] 
            print(f"Loaded node positions with shape: {node_positions.shape}")

        # [Instruction] 3c: Perform your custom calculation
        # This is where your unique analysis logic goes.
        print("Calculating custom metric...")
        my_metric = np.max(np.abs(target - prediction))
        
        # [Instruction] 3d: Return the result object
        # Instantiate your result class with the calculated values.
        return MyBenchmarkResult(experiment_dir, my_metric)


# [Instruction] Step 4: How to use your new benchmark
#
# In your main script (like `run_reservoir_pipeline.py`):
#
# 1. Import your new class:
#    from my_analysis import MyBenchmark
#
# 2. Instantiate it:
#    my_benchmark = MyBenchmark()
#
# 3. Pass it to the pipeline:
#    run_pipeline(..., benchmark=my_benchmark)
"""

def create_benchmark(args):
    """Creates a new benchmark file from the template."""
    filepath = Path(args.filepath)
    
    # Add .py extension if not present
    if filepath.suffix != '.py':
        filepath = filepath.with_suffix('.py')

    if filepath.exists():
        print(f"Error: File already exists at {filepath}", file=sys.stderr)
        sys.exit(1)

    try:
        # Use strip() to remove leading/trailing whitespace from the template string
        filepath.write_text(TEMPLATE_CONTENT.strip() + '\n')
        print(f"Successfully created benchmark template at: {filepath.resolve()}")
    except IOError as e:
        print(f"Error writing to file: {e}", file=sys.stderr)
        sys.exit(1)

def main():
    """Main entry point for the OpenPRC command-line interface."""
    parser = argparse.ArgumentParser(
        description="OpenPRC command-line tools.",
        formatter_class=argparse.RawTextHelpFormatter
    )
    subparsers = parser.add_subparsers(dest='command', required=True)

    # --- create-benchmark command ---
    parser_create = subparsers.add_parser(
        'create-benchmark',
        help='Create a new benchmark file from a template.',
        description='''
Creates a new Python file containing a boilerplate template for a custom benchmark.
This makes it easy to add your own analysis to the OpenPRC pipeline.

Example:
  openprc create-benchmark my_new_analysis
'''
    )
    parser_create.add_argument(
        'filepath',
        type=str,
        help='The path for the new benchmark file (e.g., "my_analysis.py").'
    )
    parser_create.set_defaults(func=create_benchmark)

    if len(sys.argv) == 1:
        parser.print_help(sys.stderr)
        sys.exit(1)

    args = parser.parse_args()
    args.func(args)

if __name__ == "__main__":
    main()
